## DataFlow and TensorFlow
Tensors are multidimensional mathematical objects
Rank n = 0. a number
Rank n = 1, a list of numbers
Rank n = 2., a list of lists

### TensorFlow
- Construct a dataflow graph
- Run the graph using optimized C++ code
#### Reverse - Mode Autodiff
Implements automatic derivatives of any functions:
- Forward pass (from inputs to outputs)
- Backward pass (allows to computer derivates sequentially)
- Use a chain rule for a composite function

##### Initializing
```Python
import tensorflow as tf

x = tf.Variable(3, name="x")
y = tf.Variable(4, name="y")
a = tf.constant(2, name="a")

f = x*x*y + y + a

# f at this point displays lazy evalutaion
```

##### Evaluation - a verbose way
```Python
# to evaluate the graph, open a session, initializes all variables and evaluates
sess = tf.Session()
sess.run(x.initializer)
sess.run(y.initializer)
result = sess.run(f)
a_val = a.eval(session=sess) # constant needs not be initialized

sess.close()

# a = 2
# result = 42
```
###### same as
```Python
# automatic closing
with tf.Session() as sess:
    x.initializer.run()
    y.initializer.run()
    result = f.eval() # same as sess.run(f)
    a_val = a.eval()
```

##### Initialization of all variables at once
```Python
init = tf.global_variables_initializer() # prepare init node on the graph

with tf.Session() as sess:
    init.run()
    result = f.eval()
```

##### Lifecycle of a node value
- All node values are dropped between runs, except variable values
- A variable starts its life when its initializer is run and ends when the session is closed
``` Python
with tf.Session() as sess:
    print(y.eval())
    print(z.eval())

# instead, do
with tf.Session() as sess:
    y_val,z_val = sess,run([y,z])
```

##### Reverse-mode autodiff
Define a composite function
$$
f(w)=\exp(w_{20} + w_{21} \cdot \exp(w_{10}+w_{11}\cdot \exp(w_{00}+w_{01}\cdot x)))
$$
```Python
def my_func(w,x):
    f_0 = tf.exp(w[0,0] + w[0,1]*x)
    f_1 = tf.exp(w[1,0] + w[1,1]*f_0)
    f_2 = tf.exp(w[2,0] + w[2,1]*f_1)

    return f_2, f_1, f_0

# fancier implementation using name scopes
def my_func(w,x):
    with tf.name_scope("f_0_level") as scope_0:
        f_0 = tf.exp(w[0,0] + w[0,1]*x)
    with tf.name_scope("f_1_level") as scope_1;
        f_1 = tf.exp(w[1,0] + w[1,1]*f_0)
    with tf.name_scope("f_2_level") as scope_2:
        f_2 = tf.exp(w[2,0] + w[2,1]*f_1)
        
    return f_2, f_1, f_0
```

``` Python
w_0 = np.vstack((np.zeros(3), np.ones(3))).T
# w_0 is a point at which we want to compute the function and its derivatives
```
##### Compute gradients using TF
```Python
w = tf.Variable(w_0, name = "w", dtype=tf.float32)
x = tf.Variable(1,0, name="x", dtype=tf.float32, trainable = False)

f_2,f_1,f_0 = my_func(w,x)
grads = tf.gradients(f_2,w)

init = tf.global_variables_initializer()

t_0 = time.time()

with tf.Session() as sess:
    sess.run(init)
    gradients, function_vals = sess.run([grads, [f_2, f_1, f_0]])
```

## Linear Regression in TensorFlow
$$
y(x) = a+b_{1}\cdot X_{1}+b_{2}\cdot X_{2}+b_{3}\cdot X_{3}\cdot \sigma \cdot \epsilon
$$
$a = 1.0$
$b_{1},b_{2},b_{3} = (0.5,0.2,0.1)$
$\sigma = 0.1$
$X_{1},X_{2},X_{3}$ are $uniform[-1,1]$

```Python
train_test_split = 4
#1/4 of data is used for a test

# numpy
X = np.hstack((np.ones(n_train).reshape((-1,1)), X_train))
theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(X.T).dot(Y_train)

# Sklearn
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X_train, Y_train)

# TensorFlow
X_np = np.hstack((np.ones(n_train).reshape((-1,1)), X_train))

X = tf.constant(X_np, dtype=tf.float32, name = "X")
y = tf.constant(Y_np, dtype=tf.float32, name = "y)
XT = tf.transpose(X)
theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)

with tf.Session() as sess:
    theta_value = theta.eval()
```

A simple class for Linear Regression
```Python
class Model:

    def __init__(self, n_features, learning_rate=0.05, L=0):

        # input placeholder
        # tensor needs to have nodes to pass input data, so these are initialized with float and dimension of data.
        # None means any number of rows, while, it has n_feautres columns
        self.X = tf.placeholder(tf.float32, [None, n_features], name = "X")
        self.Y = tf.placeholder(tf.float32, [None, 1], name = "Y")

        # regression parameters for analytical solution using normal equation
        self.theta_in = tf.placeholder(tf.float32, [n_features+1, None])

        # augmented data matrix is obtained by adding a column of ones to the data matrix
        data_plus_bias = tf.concat([tf.ones([tf.shape(self.X)[0], 1]), self.X]. axis=1)

        XT = tf.transpose(data_plus_bias)

        # normal equation for linear regression
        self.theta = tf.matmul(tf.matmul(
            tf.matrix_inverse(tf.matmul(XT, data_plus_bias)), XT), self.Y)

        # mean square error in terms of theta = theta_in
        self.lr_mse = tf.reduce_mean(tf.square(
            tf.matmul(data_plus_bias, self.theta)in) - self.Y)
    
```

## Neural Networks
Linear Regression can be written as a functional transforms, where it is one node computing a linear function of inputs