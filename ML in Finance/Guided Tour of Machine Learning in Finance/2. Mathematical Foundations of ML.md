## Generalization and a Bias-Variance Tradeoff
Generalization in learning is to perform well under unseen data
#### Generalization Error in Regression
$$
y=f(x)+ \epsilon
$$
Here $\mathbb{E}[\epsilon]=0, \mathbb{E}[\epsilon^2]=\sigma^2$ (error term) and $x$ is a vector
Which produces $\mathbb{E}[y]=f(x)$, $Var(y)=\sigma^2$

We look for an approximation $f(x) \approx \hat{f}(x)$ such that the generalization error (expected loss) $\mathbb{E}[(y-\hat{f}(x))^2]$ is minimized of all possible data, both seen and unseen
$$
\mathbb{E}[(y-\hat{f}(x))^2]=\mathbb{E}[y^2]+\mathbb{E}[\hat{f}^2]-2\mathbb{E}[y\hat{f}]
$$
Take all possible combinations of $x$ and $\epsilon$ to get the expectation
Evaluating the function gives,
$$
=Var(y)+\mathbb{E}[y]^2+Var(\hat{f})+\mathbb{E}[\hat{f}]^2-2\mathbb{E}[y\hat{f}]
$$
$$
=Var(y)+Var(\hat{f})+\mathbb{E}[f-\hat{f}]^2
$$
This is the bias-variance decomposition
$$
\mathbb{E}[(y-\hat{f}(x))^2]=(\text{bias})^2+\text{variance}+\text{noise}
$$
where
$(\text{bias})^2 = \mathbb{E}[f-\hat{f}]^2$
- Expected difference of approximate predicator from the true predictor
$\text{variance}=Var(\hat{f})$
- Sensitivity of $\hat{f}(x)$ to the choice of data set
$\text{noise}=Var(y)=\sigma^2$
- a property of data; beyond our control
##### Bias-variance tradeoff
- We do not know how to compute bias and variance
- Shows a general decomposition of expected loss of a generic regression model
- Complex models (more features) tend to have low bias and high variance
    - More capacity to fit the data
- Simple models (less features) tend to have high bias and low variance
- It is important to control model complexity to have an optimal tradeoff

## No Free Lunch Theorem
No single ML classification algorithm can be universally better than any other one on all domains

2 models, one sophisticated and one not, will have the same error rate if their performance is averaged over all possible data sets of the same size

Look for ML algorithms that perform best of certain classes of domains (characterized by a data-generating distribution $p_{\text{data}}$)

## Overfitting and Model Capacity
#### Training Set and Test Set

Originally you have a design matrix with dimension $(N *D)$, $D$ features with $N$ rows
Each record will have an output $Y$ with length $N$
Now, split the design matrix into 2 matrices, $N_{\text{train}}$ and $N_{\text{test}}$. This will output 2 matrices $Y_{\text{train}}$ and $Y_{\text{test}}$ with lengths $N_{\text{train}}$ and $N_{\text{test}}$.
- Pairs of $N$ and $Y$ are iid samples from an unknown data generating distribution $p_{\text{data}}$

A model is trained using only a training set $(X_{\text{train}},y_{\text{train}}) \sim p_{\text{data}}$
A test set is used to estimate the algorithm's ability to generalize/perform well on unseen data
- More specifically, a test set is used to detect when a ML algorithm starts to overfit data by estimating a generalization error by a test error

#### Overfitting
By making a model progressively more complex, you can eventually fit any training set arbitrarily well.

Initially, the test error will drop when model complexity increases, but will reach an inflection point at which more model complexity will cause more test error. To the right of this inflection point is over-fitting (high variance). To the left is under-fitting with high bias.
- The minimum error will balance the model complexity and data complexity

A good ML algorithm will 
1. Make the training error small to avoid under-fitting
2. Make the gap between training and test errors small to avoid over-fitting

## Linear Regression
Task: predict a scalar value $y \in \mathbb{R}$ from a vector of predictors ("features") 
$X=(X_{0},X_{1},\dots ,X_{D-1}) \in \mathbb{R}^D$ ($D\geq 1$ is the dimension of the feature space, or the number of predictors)

Given: a dataset $(X,y)_{\text{data}}=[(X_{\text{train}},y_{\text{train}}),(X_{\text{test}}, y_{\text{test}})] \sim p_{\text{data}}$
Architecture: Linear
$$
\hat{y}=XW
$$
$\hat{y}$ is a vector of $N$ predicted values
$X$ is a $N*D$ design matrix
$W \in \mathbb{R}^D$ is a vector of parameters

Performance measure $P$: mean square error on the **test** set:
$$
MSE_{\text{test}}=\frac{1}{N}\sum_{n=1}^N (\hat{y}_{i}-y_{i})^2 = \frac{1}{N} ||\hat{Y}-Y ||^2_{2} = \frac{1}{N}||X^\text{test}W-Y^{\text{test}}||^2_{2}
$$
Mean value of squared differences between predicted $\hat{y}$ and real $y$. Can also be expressed as the rescaled squared Euclidian distance between vectors $\hat{Y}$ and $Y$

Optimize parameters $W \in \mathbb{R}^D$ by minimizing MSE on the **training** set
because both $MSE_{\text{train}}$ and $MSE_{\text{test}}$ estimate the same generalization error $\mathbb{E}[(\hat{y}-y)^2]$ from the empirical distribution $\sim p_{data}$

For optimal weights $W$, the gradient of $MSE_{train}$ should be 0:
$$
\nabla_{W}MSE_{train}=\nabla_{W} \frac{1}{N_{train}}||\hat{Y}^{train}-Y^{train}||^2_{2}=\frac{1}{N_{train} }\nabla_{W}||X^{train}W-Y^{train}||^2_{2}=0
$$
$$
\implies \nabla_{W}(XW-y)^T(XW-y)=0
$$
$$
\implies \nabla _{W}(W^TX^TXW-2W^TX^Ty+y^Ty)=2(X^TX)W-2(X^Ty)=0
$$
$$
\implies W=(X^TX)^{-1}X^Ty
$$
Prediction in-sample
$$
\hat{y}=XW=X(X^TX)^{-1}X^Ty = Hy
$$
Matrix $H$ is called the hat (projection) matrix
- W contains the inverse of the product X and its transpose. If the columns are nearly identical or very strongly correlated, that can lead to $XW$ being a degenerate matrix.
- This can also happen when one column is a linear combination of other columns
- Determinant very close to 0, can cause numerical instability and infinities
## Regularization, Validation set, and Hyper-parameters
#### Regularization
We minimize $MSE_{train}$ through we want to minimize $MSE_{test}$. The idea of regularization is to modify the objective function of minimization of $MSE_{train}$ so that $MSE_{test}$ has a smaller variance
$$
J(W)=MSE_{train}(W)+\lambda \Omega(W)
$$
This is the regularized loss function.
$MSE_{train}$ is the Train loss
$\Omega(W)$ is a regularizer function with a regularization parameter $\lambda$. $\Omega(W)$ a function of model parameters $W$ and not the data.
We want to choose an optimal value of $\lambda$ to give the correct weight for the data and the parameters

Popular choices for the regularizer function:
- $L_{2}$ regularization $\Omega(W)=W^TW=||W||_{2}$
    - Penalizes large weights
- $L_{1}$ regularization $\Omega(W)=\sum_{i}|W_{i}|=||W||_{1}$
    - Enforces a sparse solution
- Entropy regularization $\Omega(W) = \sum_{i}W_{i}\log W_{i} \quad (W_{i}\geq 0, \sum_{i}W_{i}=1)$
    - Motivated by Bayesian statistics

#### Hyperparameters and Validation Set
- Hyperparameters are any quantitative features of ML models that are not directly optimized by minimizing in-sample loss such as $MSE_{train}$
- Hyperparameters are used to control model capacity
    - The ability of the model to adjust to progressively complex data
- Examples of hyperparameters:
    - Degree of a polynomial regression (linear, quadratic, cubic, etc.)
    - Regularization parameter $\lambda$
    - Number of levels in a decision tree
    - Number of layers and nodes per layer in neural networks
    - Learning rate

Hyperparameters are chosen by
1. Splitting a training set into training and validation sets (e.g. 80:20)
    1. Training set trains the parameters while the validation sets train the hyperparameters
    2. The final performance of the model is trained by the test set
    3. Might not be ideal when data set is small
2. Use cross-validation
#### Cross Validation
Assume we are given $N$ samples but $N$ is small, so setting aside a fixed test set is problematic
###### K-fold cross-validation
- Define a small range of possible values for the hyperparameters, so you only have to select the best candidate
- Partition our training dataset into $K$ blocks, $X_{1},X_{2},\dots,X_{K}$ of equal size
- Take the first block out and train the model on the rest of the data
- Evaluate the model error for block $X_{1}$ using the current value of the hyperparameter
- Re-insert block 1, remove block $X_{2}$ and repeat the process for all $K$ blocks
- Compute the average out-of-sample error, and do this for all possible values of the hyperparameter we want to try.
- The best value will provide the smallest average out-of-sample-error

5 and 10 are popular numbers for $K$